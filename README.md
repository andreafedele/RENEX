# RENEX: Relation Network Explainer for Fewâ€‘Shot Learning

![RENEX explanation example](renex.png)
RENEX explanation for a 5â€‘way 1â€‘shot classification task on the `Caltech-UCSD Birds 200` dataset. **Top row:** the query image $x$ (far left) followed by support samples $s_1$ â€“ $s_5$ (left to right), with each pairâ€™s RN relation score displayed above. **Bottom row:** corresponding explanation heatmaps $e_1$ â€“ $e_5$, where red indicates positive contributions, blue negative, and gray neutral. Class labels $c_1$ â€“ $c_5$ are Summer Tanager, Field Sparrow, Orchard Oriole, White Pelican, and Redâ€‘legged Kittiwake, respectively.

## ğŸ” Introduction

RENEX (RElation Network EXplainer) is a local, postâ€‘hoc explainer designed to shed light on the decisionâ€‘making process of Relation Networks in nâ€‘way 1â€‘shot fewâ€‘shot learning. By systematically perturbing superpixel regions of each support image and measuring changes in the networkâ€™s relation score, RENEX produces fineâ€‘grained heatmaps that highlight positive (red) and negative (blue) contributions to a given prediction. RENEX works outâ€‘ofâ€‘theâ€‘box on both RGB (miniImageNet, CIFAR100, CUBâ€‘200â€‘2011) and grayscale (Omniglot) benchmarks, and is fast enough for realâ€‘time deployment.

## ğŸš€ Features

- **Modelâ€‘agnostic**: Works with any pretrained Relation Network (RN) architecture.  
- **Fast**: Generates explanations in â‰ˆ0.05â€“0.07â€¯s per support/query pair.  
- **Accurate**: Outperforms LIME and Integrated Gradients on insertion/deletion metrics.  
- **RGB & Grayscale**: Easily switch between color datasets (`rgb`) and blackâ€‘&â€‘white (`bw`).  

## ğŸ“¥ Data Preparation

1. **miniImageNet**  
   - Download the Ravi &â€¯Larochelle split from  
     `https://yaoyaoliu.web.illinois.edu/projects/mtl/download/Lmzjm9tX.html`  
   - Unzip into `data/Imagenet/`. No further preprocessing required.

2. **CIFARâ€‘100**  
   - Download from the same link as miniImageNet (split available).  
   - Place under `data/CIFAR100/`.  

3. **CUBâ€‘200â€‘2011**  
   - Download images & annotations from  
     `https://www.vision.caltech.edu/datasets/cub_200_2011/`  
   - Unpack into `data/CUB100/` and run:
     ```bash
     python data/cub_processing.py
     ```

4. **Omniglot**  
   - Clone or download from `https://github.com/brendenlake/omniglot`.  
   - Merge `images_background/` and `images_evaluation/` into `data/Omniglot/`.  

## âš™ï¸ Configuration (`settings.py`)

Edit `settings.py` to switch datasets, adjust training/testing regimes, and tune hyperparameters.

## ğŸ’» Code Base Usage

This codebase provides end-to-end training and evaluation of a Relation Network for an nâ€‘way kâ€‘shot task via the `main` script, through the `mode` parameter in the `settings.py` file.

## ğŸ“ŠÂ Result Reproduction
All quantitative and qualitative figures (AUC scores, heatmaps) from the paper can be regenerated by:
1. Ensuring your setting match those in the paper's Section V (segmenter, replacement colors, seeds);
2. Running both `experiments.py` and `experiments_gradients.py`;
3. Inspecting the output csv and saved heatmaps in `results/`.
